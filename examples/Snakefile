import os
from snakemake.utils import report

# ========================================================================================
# Pipeline by Marko Fritz <marko.fritz@embl.de>, Thomas Schwarzl <schwarzl@embl.de>, Nadia Ashraf <nadia.ashraf@embl.de>
# __author__  = "Marko Fritz, Thomas Schwarzl, Nadia Ashraf"
# __license__ = "EMBL"
# ========================================================================================
# Options and paths

"""
htseq-CLIP workflow

Processing of eCLIP data in standard ENCODE format. 

The input are sorted duplicated removed bam files and an annotation file in
gff3 / gtf (gencode) format
"""

# ----------------------------------------------------------------------------------------
# CONFIG FILE
# ----------------------------------------------------------------------------------------
# This is the path to the config file. If you do not want to use the default config 
# specified here, the config file can be manually set when executing snakemake using 
# the --configfile parameter. 
#
#  --configfile FILE     Specify or overwrite the config file of the workflow
#                       (see the docs). Values specified in JSON or YAML
#                        format are available in the global config dictionary
#                        inside the workflow.
#                        
# Location of the default config file

configfile: "config.json"



# ----------------------------------------------------------------------------------------
# TOOL PATHS
# ----------------------------------------------------------------------------------------

BAM_DIR     = config["bam_input_dir"]
BAM_PREFIX  = config["bam_prefix"]
BAM_SUFFIX  = config["bam_suffix"]

OUT_DIR       = config["output_dir"]
GTF          = config["gtf_file"]
GTF_NAME     = config["gtf_short_name"]

FASTA        = config["fasta"]
FASTA_NAME       = config["fasta_name"]
CLIP         = config["clip"]
LOG_DIR          = config["log_dir"]
BED_DIR       = config["beddir"] # why do we need this
SITES        = config["sites_to_extract"].split()

# annotation parameters
ANNOTATION_PARAMS = config["annotation_params"]
CROSSLINK_EXTRACTION_PARAMS = config["crosslinks_extraction_params"]

# ----------------------------------------------------------------------------------------
# DEFINE FILE PATHS
# ----------------------------------------------------------------------------------------


ANNOTATION_FILE = expand("{outdir}/gtf/{gtfn}.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR)
SAMPLES_WILDCARDS = expand("{bamdir}/{prefix}{{sample}}{suffix}",
                             bamdir=BAM_DIR,
                             prefix = BAM_PREFIX,
                             suffix = BAM_SUFFIX)

# ----------------------------------------------------------------------------------------
# SAMPLES
# ----------------------------------------------------------------------------------------
# Automatically read in all samples

SAMPLES, = glob_wildcards(expand("{sample_dir}/{prefix}{{samples}}{suffix}",
                                 prefix = BAM_PREFIX,
                                 suffix = BAM_SUFFIX,
                                 sample_dir = BAM_DIR)[0])

# ----------------------------------------------------------------------------------------	
# :::::::: ALL :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# ----------------------------------------------------------------------------------------	

print("PROCESSING samples %s" % (', '.join(SAMPLES)))
print("PROCESSING sites %s" % (', '.join(SITES)))
print ("junction count_plots read_plots junction_plot extract_SS extract_E1 dexseq ")





rule all:
	run:
		print(ANNOTATION_FILE)
		
# 
# rule all:
# 	input:
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
# 		# ::::::: PROCESS FASTA :::::::::
# 		#expand("{outdir}/fastq/{fastan}.fastq.gz", fastan=FASTA_NAME, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: PROCESS GTF :::::::::
# 		expand("{outdir}/gtf/{gtfn}.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
# 		# ::::::: SORT PROCESSED GTF :::::::::
# 		expand("{outdir}/gtf/{gtfn}.sorted.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: PROCESS SLIDING WINDOW :::::::::
# 		expand("{outdir}/gtf/{gtfn}.sw.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: SORT SLIDING WINDOW :::::::::
# 		expand("{outdir}/gtf/{gtfn}.sw.sorted.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: EXTRACT SITES :::::::::
# 		expand("{outdir}/extract/{samples}_{sites}.bed.gz", samples=SAMPLES, outdir=OUT_DIR, sites=SITES),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: COUNT :::::::::
# 		#expand("{outdir}/counts/{samples}_{sites}to{gtfn}.count.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: JUNCTION :::::::::
# 		expand("{outdir}/junction/{samples}_{sites}to{gtfn}.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: DO SLIDING WINDOW :::::::::
# 		expand("{outdir}/slidingWindow/{samples}_{sites}to{gtfn}.sw.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: DO CONVERTION TO DEXSEQ :::::::::
# 		expand("{outdir}/dexseq/{samples}_{sites}to{gtfn}.dex.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: BOKEH READ PLOTS :::::::::
# 		expand("{outdir}/plots/reads/{samples}_{sites}/{samples}_{sites}to{gtfn}_reads.html", samples=SAMPLES, gtfn=GTF_NAME, sites=SITES, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: BOKEH COUNT PLOTS :::::::::
# 		expand("{outdir}/plots/counts/{samples}_{sites}/{samples}_{sites}to{gtfn}_counts.html", samples=SAMPLES, gtfn=GTF_NAME, sites=SITES, outdir=OUT_DIR),
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 		# ::::::: BOKEH JUNCTION PLOTS :::::::::
# 		expand("{outdir}/plots/junction/{samples}_{sites}/{samples}_{sites}to{gtfn}_junction.html", samples=SAMPLES, gtfn=GTF_NAME, sites=SITES, outdir=OUT_DIR)
# 		# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# ----------------------------------------------------------------------------------------	
# FASTA
# ----------------------------------------------------------------------------------------	

# this rule takes a sequence from a fasta file and splits it into reads
# parameter: -x read length

rule process_fasta:
	input:
		fasta=FASTA
	output:
		expand("{outdir}/fastq/{fastan}.fastq.gz", fastan=FASTA_NAME, outdir=OUT_DIR)
	log:
		expand("logs/{fastan}.log", fastan=FASTA_NAME, outdir=OUT_DIR)
	params:
		"-x 42"
	message:
		"Splitting fasta file into reads"
	shell:
		"{CLIP} genomeToReads -i {input.fasta} -o {output} {param} 2> {log}"
				
# ----------------------------------------------------------------------------------------	
# GTF
# ----------------------------------------------------------------------------------------	

		
rule annotation:
	input:
		gtf=GTF
	output:
		expand("{outdir}/gtf/{gtfn}.unsorted.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR)
	log:
		expand("{logs}/annotation_{gtfn}_preprocessing.log", gtfn=GTF_NAME, outdir=OUT_DIR, logs = LOG_DIR)
	params:
		ANNOTATION_PARAMS 
	message:
		"Preprocessing annotation {input}"
	shell:
		"{CLIP} annotation -g {input.gtf} -o {output} {params} 2> {log}"
		

rule sort_annotation:
	input:
		expand("{outdir}/gtf/{gtfn}.unsorted.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR)
	output:
		ANNOTATION_FILE
	log:
		expand("{logs}/annotation_{gtfn}_sort.log", gtfn=GTF_NAME , outdir=OUT_DIR, logs = LOG_DIR)
	message:
		"Sorting preprocessed annotation {input}"
	shell:
		"zcat {input} | sort -k1,1 -k2,2n | gzip > {output} 2> {log}"
		
# 				
# rule sliding_window:
# 	input:
# 		ANNOTATION_FILE
# 	output:
# 		expand("{outdir}/gtf/{gtfn}.sw.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR)
# 	log:
# 		expand("{logs}/{gtfn}.sw.log", gtfn=GTF_NAME, outdir=OUT_DIR, logs = LOG_DIR)
# 	shell:
# 		"{CLIP} slidingWindow -i {input} -o {output} -w 50 -s 20 2> {log}"
# 		
# rule sort_sliding_window:
# 	input:
# 		expand("{outdir}/gtf/{gtfn}.sw.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR)
# 	output:
# 		expand("{outdir}/gtf/{gtfn}.sw.sorted.bed.gz", gtfn=GTF_NAME, outdir=OUT_DIR)
# 	log:
# 		expand("{logs}/{gtfn}.sw.sorted.log", gtfn=GTF_NAME , outdir=OUT_DIR, logs = LOG_DIR)
# 	shell:
# 		"zcat {input} | sort -k1,1 -k2,2n | gzip > {output} 2> {log}"
# 		
				
# ----------------------------------------------------------------------------------------	
# EXTRACT
# ----------------------------------------------------------------------------------------	

#START SITES
rule extract_SS:
	input:
		expand("{outdir}/extract/{samples}_SS.bed.gz", samples=SAMPLES, outdir=OUT_DIR)
		
rule do_extract_SS:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_SS.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_SS.extract.log", logs = LOG_DIR)
	params:
		CROSSLINK_EXTRACTION_PARAMS
	shell:
		"{CLIP} extract -i {input} -o {output} -c s {params} 2> {log}"

#START-1 SITES
rule extract_S1:
	input:
		expand("{outdir}/extract/{samples}_S1.bed.gz", samples=SAMPLES, outdir=OUT_DIR)
		
rule do_extract_S1:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_S1.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_S1.extract.log", logs=LOG_DIR)
	params:
		CROSSLINK_EXTRACTION_PARAMS
	shell:
		"{CLIP} extract -i {input} -o {output} -c s-1 {params} 2> {log}"

#MIDDLE SITES
rule extract_MS:
	input:
		expand("{outdir}/extract/{samples}_MS.bed.gz", samples=SAMPLES, outdir=OUT_DIR)
		
rule do_extract_MS:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_MS.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_MS.extract.log", logs=LOG_DIR)
	params:
		CROSSLINK_EXTRACTION_PARAMS
	shell:
		"{CLIP} extract -i {input} -o {output} -c m {params} 2> {log}"

#END SITES
rule extract_ES:
	input:
		expand("{outdir}/extract/{samples}_ES.bed.gz", samples=SAMPLES, outdir=OUT_DIR)
		
rule do_extract_ES:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_ES.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_ES.extract.log", logs = LOG_DIR)
	params:
		CROSSLINK_EXTRACTION_PARAMS
	shell:
		"{CLIP} extract -i {input} -o {output} -c e {params} 2> {log}"

#END SITES plus offset
rule extract_E1:
	input:
		expand("{outdir}/extract/{samples}_E1.bed.gz", samples=SAMPLES, outdir=OUT_DIR)
		
rule do_extract_E1:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_E1.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_E1.extract.log", logs = LOG_DIR)
	params:
		CROSSLINK_EXTRACTION_PARAMS
	message:
		"Extracting crosslink sites from {input}"
	shell:
		"{CLIP} extract -i {input} -o {output} -c e {params} 2> {log}"
		
#DELETION SITES
rule extract_DEL:
	input:
		expand("{outdir}/extract/{samples}_DEL.bed.gz", samples=SAMPLES, outdir=OUT_DIR)
		
rule do_extract_DEL:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_DEL.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_DEL.extract.log", logs = LOG_DIR)
	shell:
		"{CLIP} extract -i {input} -o {output} -c d 2> {log}"

#INSERTION SITES
rule extract_INS:
	input:
		expand("{outdir}/extract/{samples}_INS.bed.gz", samples=SAMPLES, outdir=OUT_DIR)

		
rule do_extract_INS:
	input:
		SAMPLES_WILDCARDS
	output:
		temp("{OUT_DIR}/extract/{sample}_INS.temporary.bed.gz")
	log:
		expand("{logs}/{{sample}}_INS.extract.log", logs = LOG_DIR)
	shell:
		"{CLIP} extract -i {input} -o {output} -c i 2> {log}"
		
# ----------------------------------------------------------------------------------------	
# SORT EXTRACTED SITES
# ----------------------------------------------------------------------------------------

rule sort:
	input:
		expand("{outdir}/extract/{samples}_{sites}.bed.gz",
		         samples=SAMPLES, outdir=OUT_DIR, sites=SITES)

rule do_sort:		
	input:
		"{OUT_DIR}/extract/{sample}_{site}.temporary.bed.gz"
	output:
		"{OUT_DIR}/extract/{sample}_{site,\w+}.bed.gz"
	log:
		expand("{logs}/{{sample}}_{{site}}.extract.sort.log", logs = LOG_DIR)
	message:
		"sorting {input}"
	shell:
		"zcat {input} | sort -k1,1 -k2,2n | gzip > {output} 2> {log}"
		
# ----------------------------------------------------------------------------------------
# BIGWIG create:
# ----------------------------------------------------------------------------------------

rule chrom_sizes:
	input:
		expand("{outdir}/bedTobig/{sample}.chromsize", sample=SAMPLES,  outdir=OUT_DIR)
 
rule do_chrom_sizes:
	input:
		SAMPLES_WILDCARDS
	output:
		expand("{outdir}/bedTobig/{{sample}}.chromsize", outdir=OUT_DIR)
	message:
		"extracting chromosome sizes for {wildcards.sample}"
	shell:
		"""samtools idxstats {input} | perl -alne 'print "$F[0]\t$F[1]" if $F[0]!~/\*/' > {output}"""

# Create befgraphs
rule bedgraph:
	input:
		expand("{outdir}/bedTobig/{sample}.bedgraph",sample=SAMPLES, outdir=OUT_DIR)

rule do_bedgraph:
	input:
		bed = expand("{bed_dir}/{{sample}}.bed", bed_dir = BED_DIR),
		chromsizes = expand("{outdir}/bedTobig/{{sample}}.chromsize", outdir = OUT_DIR)
	output:
		expand("{outdir}/bedTobig/{{sample}}.bedgraph", outdir=OUT_DIR)
	message:
		"Create bedgraphs for sample {wildcards.sample}"
	shell:
		"""genomeCoverageBed -bg -split -i {input.bed} -g {input.chromsizes}  | perl -alne '$"="\t"; $F[-1]=int($F[-1]+0.5); print "@F"'> {output}"""

# Create BigWig
rule bigwig:
	input: 
		expand("{outdir}/bedTobig/{sample}.bw", outdir=OUT_DIR,sample=SAMPLES)

rule do_bigwig:
	input:
		bedgraph = expand("{outdir}/bedTobig/{{sample}}.bedgraph", outdir=OUT_DIR),
		chromsize = expand("{outdir}/bedTobig/{{sample}}.chromsize", outdir=OUT_DIR)
	output:
		expand("{outdir}/bedTobig/{{sample}}.bw",outdir=OUT_DIR  )
	message:
		"Create BigWigs for sample {wildcards.sample}"
	shell:
		"bedGraphToBigWig {input.bedgraph} {input.chromsize} {output}"
		

		
# ----------------------------------------------------------------------------------------
# JUNCTION
# ----------------------------------------------------------------------------------------

rule junction:
	input:
		expand("{outdir}/junction/{samples}_{sites}to{gtfn}.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES)
		
rule do_junction:
	input:
		bed = "{OUT_DIR}/extract/{sample}_{site}.bed.gz",
		gtf = ANNOTATION_FILE
	output:
		"{OUT_DIR}/junction/{sample}_{site}to{gtfn}.txt.gz"
	log:
		expand("{logs}/{{sample}}_{{site}}.junction.log", logs = LOG_DIR)
	shell:
		"{CLIP} junction -i {input.bed} -f {input.gtf} -o {output} 2> {log}"
		
		
# ----------------------------------------------------------------------------------------
# COUNT
# ----------------------------------------------------------------------------------------

rule count:
	input:
		expand("{outdir}/counts/{samples}_{sites}to{gtfn}.count.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES)
		
rule do_count:
	input:
		bed = expand("{outdir}/extract/{{sample}}_{{site}}.bed.gz", outdir = OUT_DIR), 
		gtf = ANNOTATION_FILE
	output:
		expand("{outdir}/counts/{{sample}}_{{site}}to{{gtfn}}.count.txt.gz", outdir = OUT_DIR)
	log: 
		expand("{logs}/{{sample}}_{{site}}.count.log", logs = LOG_DIR)
	message:
		"counting {input.bed}"
	shell:
		"{CLIP} count -i {input.bed} -f {input.gtf} -o {output} -c o  2> {log}"
		
		
# ----------------------------------------------------------------------------------------
# SLIDING WINDOW
# ----------------------------------------------------------------------------------------

rule sw:
	input:
		expand("{outdir}/slidingWindow/{samples}_{sites}to{gtfn}.sw.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES)
		
rule do_sw:
	input:
		bed = "{OUT_DIR}/extract/{sample}_{site}.bed.gz",
		gtf = "{OUT_DIR}/gtf/{gtfn}.sw.sorted.bed.gz"
	output:
		"{OUT_DIR}/slidingWindow/{sample}_{site}to{gtfn}.sw.txt.gz"
	log:
		expand("{logs}/{{sample}}_{{site}}.sw.count.log", logs = LOG_DIR)
	shell:
		"{CLIP} countSlidingWindow -i {input.bed} -f {input.gtf} -o {output} 2> {log}"
				
# ----------------------------------------------------------------------------------------
# DO CONVERTION TO DEXSEQ
# ----------------------------------------------------------------------------------------

rule dexseq:
	input:
		expand("{outdir}/dexseq/{samples}_{sites}to{gtfn}.dex.txt.gz", samples=SAMPLES, outdir=OUT_DIR, gtfn=GTF_NAME, sites=SITES)
		
rule do_dexseq:
	input:
		"{OUT_DIR}/slidingWindow/{sample}_{site}to{gtfn}.sw.txt.gz"
	output:
		"{OUT_DIR}/dexseq/{sample}_{site}to{gtfn}.dex.txt.gz"
	log:
		expand("{logs}/{{sample}}_{{site}}.dex.log", logs = LOG_DIR)
	message:
		"create dexseq format for {input}"
	shell:
		"{CLIP} toDexSeq -i {input} -o {output} 2> {log}"
		
		
# ----------------------------------------------------------------------------------------	
# BOKEH READ PLOTS
# ----------------------------------------------------------------------------------------	

rule read_plots:
	input:
		expand("{outdir}/plots/reads/{samples}_{sites}/{samples}_{sites}to{gtfn}_reads.html", samples=SAMPLES, gtfn=GTF_NAME, sites=SITES, outdir=OUT_DIR)
		
rule do_read_plot:
	input:
		"{OUT_DIR}/extract/{sample}_{site}.bed.gz"
	output:
		"{OUT_DIR}/plots/reads/{sample}_{site}/{sample}_{site}to{gtfn}_reads.html"
	log:
		expand("{logs}/{{sample}}_{{site}}.rplot.log", logs = LOG_DIR)
	message:
		"creating read plot for {input}"
	shell:
		"PYTHONPATH="" "
		"{CLIP} plot -i {input} -o {output} -c r 2> {log}"
		
# ----------------------------------------------------------------------------------------	
# BOKEH COUNT PLOTS
# ----------------------------------------------------------------------------------------	

rule count_plots:
	input:
		expand("{outdir}/plots/counts/{samples}_{sites}/{samples}_{sites}to{gtfn}_counts.html", samples=SAMPLES, gtfn=GTF_NAME, sites=SITES, outdir=OUT_DIR)
		
rule do_count_plot:
	input:
		"{OUT_DIR}/counts/{sample}_{site}to{gtfn}.count.txt.gz"
	output:
		"{OUT_DIR}/plots/counts/{sample}_{site}/{sample}_{site}to{gtfn}_counts.html"
	log:
		expand("{logs}/{{sample}}_{{site}}.cplot.log", logs = LOG_DIR)
	message:
		"creating count plot for {input}"
	shell:
		"PYTHONPATH="" "
		"{CLIP} plot -i {input} -o {output} -c c 2> {log}"
		
# ----------------------------------------------------------------------------------------	
# BOKEH JUNCTION PLOTS
# ----------------------------------------------------------------------------------------	

rule junction_plot:
	input:
		expand("{outdir}/plots/junction/{samples}_{sites}/{samples}_{sites}to{gtfn}_junction.html", samples=SAMPLES, gtfn=GTF_NAME, sites=SITES, outdir=OUT_DIR)
		
rule do_junction_plot:
	input:
		"{OUT_DIR}/junction/{sample}_{site}to{gtfn}.txt.gz"
	output:
		"{OUT_DIR}/plots/junction/{sample}_{site}/{sample}_{site}to{gtfn}_junction.html"
	log:
		expand("{logs}/{{sample}}_{{site}}.jplot.log", logs = LOG_DIR)
	message:
		"creating junction plot for {input}"
	shell:
		"PYTHONPATH="" "
		"{CLIP} plot -i {input} -o {output} -c j 2> {log}"
		
